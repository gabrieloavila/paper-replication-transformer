{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DcNevGCrFm1_"
      },
      "source": [
        "# Replicando paper - An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "or-gn1sMKo_c"
      },
      "source": [
        "[Link para o Paper](https://arxiv.org/pdf/2010.11929)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MzSbc9X_FU5u"
      },
      "source": [
        "## Organizar Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip3 install torch torchvision torchaudio matplotlib tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5rTU_4ljs3DM"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "from torch import nn\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "import os\n",
        "\n",
        "import time\n",
        "\n",
        "from pathlib import Path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BNB40g5ftWkU",
        "outputId": "9007c6d0-3e0b-4b6f-856e-2bcb1d9069a2"
      },
      "outputs": [],
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "device\n",
        "print(torch.cuda.is_available(), torch.cuda.device_count())\n",
        "print(torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"no GPU\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W9A7ZpxYK8Ce"
      },
      "source": [
        "### Etapas de Treinamento"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hP4HJy_OpKF7"
      },
      "outputs": [],
      "source": [
        "from tqdm.auto import tqdm\n",
        "from typing import Dict, List, Tuple\n",
        "\n",
        "def train_step(model: torch.nn.Module,\n",
        "               dataloader: torch.utils.data.DataLoader,\n",
        "               loss_fn: torch.nn.Module,\n",
        "               optimizer: torch.optim.Optimizer,\n",
        "               device: torch.device) -> Tuple[float, float]:\n",
        "\n",
        "    # Coloca o modelo em modo de treino\n",
        "    model.train()\n",
        "\n",
        "    # Configura os valores de train loss e train accuracy\n",
        "    train_loss, train_acc = 0, 0\n",
        "\n",
        "    # Loop entre data loader e data batches\n",
        "    for batch, (X, y) in enumerate(dataloader):\n",
        "        # Envia data para o dispositivo\n",
        "        X, y = X.to(device), y.to(device)\n",
        "\n",
        "        # 1. Forward\n",
        "        y_pred = model(X)\n",
        "\n",
        "        # 2. Calcula e acumula loss\n",
        "        loss = loss_fn(y_pred, y)\n",
        "        train_loss += loss.item()\n",
        "\n",
        "        # 3. Otimizador zero grad\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # 4. Loss backward\n",
        "        loss.backward()\n",
        "\n",
        "        # 5. Optimizer step\n",
        "        optimizer.step()\n",
        "\n",
        "        # Calcula e acumula métricas de acuracia\n",
        "        y_pred_class = torch.argmax(torch.softmax(y_pred, dim=1), dim=1)\n",
        "        train_acc += (y_pred_class == y).sum().item()/len(y_pred)\n",
        "\n",
        "    # Ajusta métricas para calcular average loss e accuracy por batch\n",
        "    train_loss = train_loss / len(dataloader)\n",
        "    train_acc = train_acc / len(dataloader)\n",
        "    return train_loss, train_acc\n",
        "\n",
        "def test_step(model: torch.nn.Module,\n",
        "              dataloader: torch.utils.data.DataLoader,\n",
        "              loss_fn: torch.nn.Module,\n",
        "              device: torch.device) -> Tuple[float, float]:\n",
        "\n",
        "    # Coloca modelo em modo eval\n",
        "    model.eval()\n",
        "\n",
        "    # Configura valores de test loss e test accuracy\n",
        "    test_loss, test_acc = 0, 0\n",
        "\n",
        "    # Ativa inference context manager\n",
        "    with torch.inference_mode():\n",
        "        # Loop entre os DataLoader batches\n",
        "        for batch, (X, y) in enumerate(dataloader):\n",
        "            # Envia data para o dispositivo\n",
        "            X, y = X.to(device), y.to(device)\n",
        "\n",
        "            # 1. Forward pass\n",
        "            test_pred_logits = model(X)\n",
        "\n",
        "            # 2. Calcula e acumula loss\n",
        "            loss = loss_fn(test_pred_logits, y)\n",
        "            test_loss += loss.item()\n",
        "\n",
        "            # Calcula e acumula  accuracy\n",
        "            test_pred_labels = test_pred_logits.argmax(dim=1)\n",
        "            test_acc += ((test_pred_labels == y).sum().item()/len(test_pred_labels))\n",
        "\n",
        "    # Ajusta métricas para calcular average loss e accuracy por batch\n",
        "    test_loss = test_loss / len(dataloader)\n",
        "    test_acc = test_acc / len(dataloader)\n",
        "    return test_loss, test_acc\n",
        "\n",
        "def train(model: torch.nn.Module,\n",
        "          train_dataloader: torch.utils.data.DataLoader,\n",
        "          test_dataloader: torch.utils.data.DataLoader,\n",
        "          optimizer: torch.optim.Optimizer,\n",
        "          loss_fn: torch.nn.Module,\n",
        "          epochs: int,\n",
        "          device: torch.device) -> Dict[str, List]:\n",
        "\n",
        "    # Cria dicionário de resultados vazios\n",
        "    results = {\"train_loss\": [],\n",
        "               \"train_acc\": [],\n",
        "               \"test_loss\": [],\n",
        "               \"test_acc\": []\n",
        "    }\n",
        "\n",
        "    # Confirma que o modelo está no dispositivo\n",
        "    model.to(device)\n",
        "\n",
        "    # Loop entre etapas de treinamento e teste por um número de épocas\n",
        "    for epoch in tqdm(range(epochs)):\n",
        "        train_loss, train_acc = train_step(model=model,\n",
        "                                          dataloader=train_dataloader,\n",
        "                                          loss_fn=loss_fn,\n",
        "                                          optimizer=optimizer,\n",
        "                                          device=device)\n",
        "        test_loss, test_acc = test_step(model=model,\n",
        "          dataloader=test_dataloader,\n",
        "          loss_fn=loss_fn,\n",
        "          device=device)\n",
        "\n",
        "        # Print o que está acontecendo\n",
        "        print(\n",
        "          f\"Epoch: {epoch+1} | \"\n",
        "          f\"train_loss: {train_loss:.4f} | \"\n",
        "          f\"train_acc: {train_acc:.4f} | \"\n",
        "          f\"test_loss: {test_loss:.4f} | \"\n",
        "          f\"test_acc: {test_acc:.4f}\"\n",
        "        )\n",
        "\n",
        "        # Atualiza os dicionários\n",
        "        results[\"train_loss\"].append(train_loss)\n",
        "        results[\"train_acc\"].append(train_acc)\n",
        "        results[\"test_loss\"].append(test_loss)\n",
        "        results[\"test_acc\"].append(test_acc)\n",
        "\n",
        "    # Returna o resultado preenchido no final das épocas\n",
        "    return results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-7SOznPEFNkQ"
      },
      "source": [
        "## Obter Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rSzAVJRavSvm",
        "outputId": "beb8a178-bb0a-4bc9-ab9d-2134562b976c"
      },
      "outputs": [],
      "source": [
        "image_path = Path(\"data/pizza_steak_sushi\")\n",
        "image_path\n",
        "train_dir = image_path / \"train\"\n",
        "test_dir = image_path / \"test\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CRvd8eGRGCJa"
      },
      "source": [
        "## Criar datasets e dataloaders"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BIh89nHaGQSj"
      },
      "source": [
        "### Preparar transformação da imagem"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5q3ZwZeCwNaq",
        "outputId": "9381b2fa-52f8-45d8-b9aa-24247fe843c1"
      },
      "outputs": [],
      "source": [
        "IMG_SIZE = 224\n",
        "manual_transforms = transforms.Compose([\n",
        "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
        "    transforms.ToTensor(),])\n",
        "print(f\"Manually created transforms: {manual_transforms}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gxcm1VmYGVqq"
      },
      "source": [
        "### Transformar imagens em dataloaders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AH46VZKJxkSk"
      },
      "outputs": [],
      "source": [
        "def create_dataloaders(\n",
        "    train_dir: str,\n",
        "    test_dir: str,\n",
        "    transform: transforms.Compose,\n",
        "    batch_size: int,\n",
        "    num_workers: int=os.cpu_count()\n",
        "):\n",
        "  # Use ImageFolder to create dataset(s)\n",
        "  train_data = datasets.ImageFolder(train_dir, transform=transform)\n",
        "  test_data = datasets.ImageFolder(test_dir, transform=transform)\n",
        "\n",
        "  # Get class names\n",
        "  class_names = train_data.classes\n",
        "\n",
        "  # Turn images into data loaders\n",
        "  train_dataloader = DataLoader(\n",
        "      train_data,\n",
        "      batch_size=batch_size,\n",
        "      shuffle=True,\n",
        "      num_workers=num_workers,\n",
        "      pin_memory=True,\n",
        "  )\n",
        "  test_dataloader = DataLoader(\n",
        "      test_data,\n",
        "      batch_size=batch_size,\n",
        "      shuffle=False,\n",
        "      num_workers=num_workers,\n",
        "      pin_memory=True,\n",
        "  )\n",
        "\n",
        "  return train_dataloader, test_dataloader, class_names"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DNa8O0sixP9J",
        "outputId": "db39a4de-b166-4420-8c75-df9da4a98326"
      },
      "outputs": [],
      "source": [
        "BATCH_SIZE = 32\n",
        "train_dataloader, test_dataloader, class_names = create_dataloaders(\n",
        "    train_dir=train_dir,\n",
        "    test_dir=test_dir,\n",
        "    transform=manual_transforms,\n",
        "    batch_size=BATCH_SIZE\n",
        ")\n",
        "\n",
        "train_dataloader, test_dataloader, class_names"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "npCvrEEd0ESU",
        "outputId": "e6615e99-e1a5-46fd-c243-34ffc4a0ec71"
      },
      "outputs": [],
      "source": [
        "# Pega um conjunto de imagens\n",
        "image_batch, label_batch = next(iter(train_dataloader))\n",
        "\n",
        "# Seleciona uma imagem do conjunto\n",
        "image, label = image_batch[0], label_batch[0]\n",
        "\n",
        "# Vê os labels das imagens\n",
        "image.shape, label"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hFfY0oBNiBc2"
      },
      "source": [
        "## Calcular e criar os patches"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SA0pHRFL1I0K",
        "outputId": "74a14fcc-cf60-48bc-f644-3b2b7381649c"
      },
      "outputs": [],
      "source": [
        "# Criar valores de exemplos segundo o paper\n",
        "height = 224 # H (\"A resolução do treino é de 224.\")\n",
        "width = 224 # W\n",
        "color_channels = 3 # C\n",
        "patch_size = 16 # P\n",
        "\n",
        "# Calcular N (número de patches)\n",
        "number_of_patches = int((height * width) / patch_size**2)\n",
        "print(f\"Número de patches(N) com altura da imagem (H={height}), largura (W={width}) e tamanho do patch (P={patch_size}): {number_of_patches}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xp0FmUNK8wKR",
        "outputId": "8429af23-5149-4c61-da53-e99a50ba9d30"
      },
      "outputs": [],
      "source": [
        "# Input shape (esse é o tamanho de uma única imagem)\n",
        "embedding_layer_input_shape = (height, width, color_channels)\n",
        "\n",
        "# Output shape\n",
        "embedding_layer_output_shape = (number_of_patches, patch_size**2 * color_channels)\n",
        "\n",
        "print(f\"Input shape: {embedding_layer_input_shape}\")\n",
        "print(f\"Output shape: {embedding_layer_output_shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XB2a8tis0J01",
        "outputId": "7bf66269-d354-44b3-cf31-ff8d32eebfaa"
      },
      "outputs": [],
      "source": [
        "# Plota uma imagem usando matplotlib\n",
        "plt.imshow(image.permute(1, 2, 0))\n",
        "plt.title(class_names[label])\n",
        "plt.axis(False);"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TcquPjhN9HeD",
        "outputId": "47e6b016-7cc1-4e00-86a2-8dcda89f4511"
      },
      "outputs": [],
      "source": [
        "# Muda a imagem para ser compatível com o matplotlib (color_channels, height, width) -> (height, width, color_channels)\n",
        "image_permuted = image.permute(1, 2, 0)\n",
        "\n",
        "# Index to plot the top row of patched pixels\n",
        "patch_size = 16\n",
        "plt.figure(figsize=(patch_size, patch_size))\n",
        "plt.imshow(image_permuted[:patch_size, :, :]);"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8etHnXyf9QvD",
        "outputId": "4db7f265-7bd4-40c4-894b-cd755382170e"
      },
      "outputs": [],
      "source": [
        "# Setup hyperparameters and make sure img_size and patch_size are compatible\n",
        "img_size = 224\n",
        "patch_size = 16\n",
        "num_patches = img_size/patch_size\n",
        "assert img_size % patch_size == 0, \"Image size must be divisible by patch size\"\n",
        "print(f\"Number of patches per row: {num_patches}\\nPatch size: {patch_size} pixels x {patch_size} pixels\")\n",
        "\n",
        "# Cria uma série de subplots\n",
        "fig, axs = plt.subplots(nrows=1,\n",
        "                        ncols=img_size // patch_size, # 1 coluna para cada patch\n",
        "                        figsize=(num_patches, num_patches),\n",
        "                        sharex=True,\n",
        "                        sharey=True)\n",
        "\n",
        "# Intera entre o número de patches na primeira coluna\n",
        "for i, patch in enumerate(range(0, img_size, patch_size)):\n",
        "    axs[i].imshow(image_permuted[:patch_size, patch:patch+patch_size, :]); # mantém index da altura constante, muda o index do comprimento\n",
        "    axs[i].set_xlabel(i+1) # adiciona a legenda\n",
        "    axs[i].set_xticks([])\n",
        "    axs[i].set_yticks([])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sh1Re-d39k48",
        "outputId": "4a0dbb47-5fd5-48a4-bfdc-97afd4780204"
      },
      "outputs": [],
      "source": [
        "# Configura os hyperparameters e mantém a certeza do img_size e patch_size são compatíveis\n",
        "img_size = 224\n",
        "patch_size = 16\n",
        "num_patches = img_size/patch_size\n",
        "assert img_size % patch_size == 0, \"Image size must be divisible by patch size\"\n",
        "print(f\"Number of patches per row: {num_patches}\\\n",
        "        \\nNumber of patches per column: {num_patches}\\\n",
        "        \\nTotal patches: {num_patches*num_patches}\\\n",
        "        \\nPatch size: {patch_size} pixels x {patch_size} pixels\")\n",
        "\n",
        "# Cria uma série de subplots\n",
        "fig, axs = plt.subplots(nrows=img_size // patch_size,\n",
        "                        ncols=img_size // patch_size,\n",
        "                        figsize=(num_patches, num_patches),\n",
        "                        sharex=True,\n",
        "                        sharey=True)\n",
        "\n",
        "# Loop entre altura e comprimento da imagem\n",
        "for i, patch_height in enumerate(range(0, img_size, patch_size)): # itera entre altura\n",
        "    for j, patch_width in enumerate(range(0, img_size, patch_size)): # itera entre comprimento\n",
        "\n",
        "        # Plota a imagem (image_permuted -> (Height, Width, Color Channels))\n",
        "        axs[i, j].imshow(image_permuted[patch_height:patch_height+patch_size, # itera entre altura\n",
        "                                        patch_width:patch_width+patch_size, # itera entre comprimento\n",
        "                                        :]) # todos os canais de cores\n",
        "\n",
        "        # Adiciona legenda\n",
        "        axs[i, j].set_ylabel(i+1,\n",
        "                             rotation=\"horizontal\",\n",
        "                             horizontalalignment=\"right\",\n",
        "                             verticalalignment=\"center\")\n",
        "        axs[i, j].set_xlabel(j+1)\n",
        "        axs[i, j].set_xticks([])\n",
        "        axs[i, j].set_yticks([])\n",
        "        axs[i, j].label_outer()\n",
        "\n",
        "# Adiciona um título\n",
        "fig.suptitle(f\"{class_names[label]} -> Patchified\", fontsize=16)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wFrEq5_k93wl"
      },
      "outputs": [],
      "source": [
        "patch_size=16\n",
        "\n",
        "# Cria a camada conv2d com os hyperparameters do ViT paper\n",
        "conv2d = nn.Conv2d(in_channels=3, # numero dos canais de cor\n",
        "                   out_channels=768, # Table 1: Hidden size D\n",
        "                   kernel_size=patch_size,\n",
        "                   stride=patch_size,\n",
        "                   padding=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P7freE3p99Ff",
        "outputId": "7fb400e4-dea8-40d4-d0d0-4b0db046df4f"
      },
      "outputs": [],
      "source": [
        "# Visualiza uma única imagem\n",
        "plt.imshow(image.permute(1, 2, 0))\n",
        "plt.title(class_names[label])\n",
        "plt.axis(False);"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jpj8SW06-Cp8",
        "outputId": "41366b46-3a92-443b-e1f3-c0df9641e6ed"
      },
      "outputs": [],
      "source": [
        "# Passa a imagem dentro da camada convolutional\n",
        "image_out_of_conv = conv2d(image.unsqueeze(0)) # adiciona uma única camada (height, width, color_channels) -> (batch, height, width, color_channels)\n",
        "print(image_out_of_conv.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qyhZB4W6jFYS"
      },
      "source": [
        "### Diminuir a dimensão"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2TliSMH0-ZRe"
      },
      "outputs": [],
      "source": [
        "# cria a camada Flatten\n",
        "flatten = nn.Flatten(start_dim=2, # flatten feature_map_height (dimension 2)\n",
        "                     end_dim=3) # flatten feature_map_width (dimension 3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0VCqCUPZ-Rww",
        "outputId": "19c60583-9ae9-41e8-901d-be77bb3c99f8"
      },
      "outputs": [],
      "source": [
        "# 1. Visualiza uma única imagem\n",
        "plt.imshow(image.permute(1, 2, 0)) #\n",
        "plt.title(class_names[label])\n",
        "plt.axis(False);\n",
        "print(f\"Original image shape: {image.shape}\")\n",
        "\n",
        "# 2. Transforma a imagem em feature maps\n",
        "image_out_of_conv = conv2d(image.unsqueeze(0))\n",
        "print(f\"Image feature map shape: {image_out_of_conv.shape}\")\n",
        "\n",
        "# 3. Flatten\n",
        "image_out_of_conv_flattened = flatten(image_out_of_conv)\n",
        "print(f\"Flattened image feature map shape: {image_out_of_conv_flattened.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rXKUbzgf-gsF",
        "outputId": "252b170d-7e65-47db-c33c-d70e5512127b"
      },
      "outputs": [],
      "source": [
        "image_out_of_conv_flattened_reshaped = image_out_of_conv_flattened.permute(0, 2, 1) # [batch_size, P^2•C, N] -> [batch_size, N, P^2•C]\n",
        "print(f\"Patch embedding sequence shape: {image_out_of_conv_flattened_reshaped.shape} -> [batch_size, num_patches, embedding_size]\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PxmYqEvH-kYC",
        "outputId": "43cd0e7e-d53f-497c-ac2b-ba2d38923952"
      },
      "outputs": [],
      "source": [
        "# Obter uma única Flatten\n",
        "single_flattened_feature_map = image_out_of_conv_flattened_reshaped[:, :, 0] # index: (batch_size, number_of_patches, embedding_dimension)\n",
        "\n",
        "# Plota o Flatten para visualização\n",
        "plt.figure(figsize=(22, 22))\n",
        "plt.imshow(single_flattened_feature_map.detach().numpy())\n",
        "plt.title(f\"Flattened feature map shape: {single_flattened_feature_map.shape}\")\n",
        "plt.axis(False);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QKJZIKn68JGp"
      },
      "source": [
        "## Recriar o Transformer do paper"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Aw9jI4DzFKgM"
      },
      "source": [
        "![vit_figure.png](https://github.com/google-research/vision_transformer/raw/main/vit_figure.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R4w5OkrQk6V7"
      },
      "outputs": [],
      "source": [
        "# Para obter uma constância\n",
        "def set_seeds(seed: int=42):\n",
        "  torch.manual_seed(seed)\n",
        "  torch.cuda.manual_seed(seed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fpJ3raNGlIkp"
      },
      "outputs": [],
      "source": [
        "class PatchEmbedding(nn.Module):\n",
        "  def __init__(self,\n",
        "                 in_channels:int=3,\n",
        "                 patch_size:int=16,\n",
        "                 embedding_dim:int=768):\n",
        "        super().__init__()\n",
        "\n",
        "        self.patcher = nn.Conv2d(in_channels=in_channels,\n",
        "                                 out_channels=embedding_dim,\n",
        "                                 kernel_size=patch_size,\n",
        "                                 stride=patch_size,\n",
        "                                 padding=0)\n",
        "\n",
        "        self.flatten = nn.Flatten(start_dim=2,\n",
        "                                  end_dim=3)\n",
        "\n",
        "  def forward(self, x):\n",
        "    image_resolution = x.shape[-1]\n",
        "    assert image_resolution % patch_size == 0, f\"Input image size must be divisible by patch size, image shape: {image_resolution}, patch size: {patch_size}\"\n",
        "\n",
        "    x_patched = self.patcher(x)\n",
        "    x_flattened = self.flatten(x_patched)\n",
        "\n",
        "    return x_flattened.permute(0, 2, 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DXDTsQpfmpj0",
        "outputId": "ac66428f-d701-42f2-bdb9-5c2327f2479f"
      },
      "outputs": [],
      "source": [
        "set_seeds()\n",
        "\n",
        "# 1. Tamanho do patch\n",
        "patch_size = 16\n",
        "\n",
        "# 2.Obter dimensões das imagens\n",
        "print(f\"Image tensor shape: {image.shape}\")\n",
        "height, width = image.shape[1], image.shape[2]\n",
        "\n",
        "# 3. Obter tensor da imagem e adiciona no batch dimension\n",
        "x = image.unsqueeze(0)\n",
        "print(f\"Input image with batch dimension shape: {x.shape}\")\n",
        "\n",
        "# 4. Cria o layer patch embedding\n",
        "patch_embedding_layer = PatchEmbedding(in_channels=3,\n",
        "                                       patch_size=patch_size,\n",
        "                                       embedding_dim=768)\n",
        "\n",
        "# 5. Passa a imagem entre o patch embedding layer\n",
        "patch_embedding = patch_embedding_layer(x)\n",
        "print(f\"Patching embedding shape: {patch_embedding.shape}\")\n",
        "\n",
        "# 6. Cria o token da classe embedding\n",
        "batch_size = patch_embedding.shape[0]\n",
        "embedding_dimension = patch_embedding.shape[-1]\n",
        "class_token = nn.Parameter(torch.ones(batch_size, 1, embedding_dimension),\n",
        "                           requires_grad=True) # tenha certeza que é aprendizável\n",
        "print(f\"Class token embedding shape: {class_token.shape}\")\n",
        "\n",
        "# 7. Class token embedding para patch embedding\n",
        "patch_embedding_class_token = torch.cat((class_token, patch_embedding), dim=1)\n",
        "print(f\"Patch embedding with class token shape: {patch_embedding_class_token.shape}\")\n",
        "\n",
        "# 8. Cria embedding de posição\n",
        "number_of_patches = int((height * width) / patch_size**2)\n",
        "position_embedding = nn.Parameter(torch.ones(1, number_of_patches+1, embedding_dimension),\n",
        "                                  requires_grad=True) # tenha certeza que é aprendizável\n",
        "\n",
        "# 9. Adiciona position embedding para patch embedding com class token\n",
        "patch_and_position_embedding = patch_embedding_class_token + position_embedding\n",
        "print(f\"Patch and position embedding shape: {patch_and_position_embedding.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NmiUNMjBBuun"
      },
      "source": [
        "### Criando o multi-head self-attention block (\"MSA block\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GDkkYXFrnMQp"
      },
      "outputs": [],
      "source": [
        "class MultiheadSelfAttentionBlock(nn.Module):\n",
        "    \"\"\"Cria o multi-head self-attention block (\"MSA block\").\n",
        "    \"\"\"\n",
        "    # 2. Inicializa a classe com os hyperparameters da Table 1\n",
        "    def __init__(self,\n",
        "                 embedding_dim:int=768, # Hidden size D da Table 1 ViT-Base\n",
        "                 num_heads:int=12, # Heads da Table 1 ViT-Base\n",
        "                 attn_dropout:float=0):\n",
        "        super().__init__()\n",
        "\n",
        "        # 3. Cria a Norm layer (LN)\n",
        "        self.layer_norm = nn.LayerNorm(normalized_shape=embedding_dim)\n",
        "\n",
        "        # 4. Cria a camada Multi-Head Attention (MSA)\n",
        "        self.multihead_attn = nn.MultiheadAttention(embed_dim=embedding_dim,\n",
        "                                                    num_heads=num_heads,\n",
        "                                                    dropout=attn_dropout,\n",
        "                                                    batch_first=True)\n",
        "\n",
        "    # 5. Cria o método forward() para passar os dados entre as camadas\n",
        "    def forward(self, x):\n",
        "        x = self.layer_norm(x)\n",
        "        attn_output, _ = self.multihead_attn(query=x,\n",
        "                                             key=x,\n",
        "                                             value=x,\n",
        "                                             need_weights=False)\n",
        "        return attn_output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ESHi0scPoG_x",
        "outputId": "7fe302bf-8626-4364-f88b-68c33a6a24c4"
      },
      "outputs": [],
      "source": [
        "# Cria a instância do MSABlock\n",
        "multihead_self_attention_block = MultiheadSelfAttentionBlock(embedding_dim=768, # from Table 1\n",
        "                                                             num_heads=12) # from Table 1\n",
        "\n",
        "# Passa o patch e position image embedding para o MSABlock\n",
        "patched_image_through_msa_block = multihead_self_attention_block(patch_and_position_embedding)\n",
        "print(f\"Input shape of MSA block: {patch_and_position_embedding.shape}\")\n",
        "print(f\"Output shape MSA block: {patched_image_through_msa_block.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "234vof9PB1bJ"
      },
      "source": [
        "### Criando a camada do normalized multilayer perceptron block (\"MLP block\")."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1ifHzkc9oMc5"
      },
      "outputs": [],
      "source": [
        "# 1. Cria a classe do nn.Module\n",
        "class MLPBlock(nn.Module):\n",
        "    \"\"\"Cria a camada do normalized multilayer perceptron block (\"MLP block\").\"\"\"\n",
        "    # 2. Inicializa a classe com os hyperparameters da Table 1 e Table 3\n",
        "    def __init__(self,\n",
        "                 embedding_dim:int=768, # Hidden Size D da Table 1\n",
        "                 mlp_size:int=3072, # MLP size da Table 1\n",
        "                 dropout:float=0.1): # Dropout da Table 3\n",
        "        super().__init__()\n",
        "\n",
        "        # 3. Cria a Norm layer (LN)\n",
        "        self.layer_norm = nn.LayerNorm(normalized_shape=embedding_dim)\n",
        "\n",
        "        # 4. Cria a Multilayer perceptron (MLP) layer(s)\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(in_features=embedding_dim,\n",
        "                      out_features=mlp_size),\n",
        "            nn.GELU(), # \"The MLP contains two layers with a GELU non-linearity (section 3.1).\"\n",
        "            nn.Dropout(p=dropout),\n",
        "            nn.Linear(in_features=mlp_size,\n",
        "                      out_features=embedding_dim),\n",
        "            nn.Dropout(p=dropout)\n",
        "        )\n",
        "\n",
        "    # 5. Cria o método forward() para passar os dados entre as camadas\n",
        "    def forward(self, x):\n",
        "        x = self.layer_norm(x)\n",
        "        x = self.mlp(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O85-cRQNoO2O",
        "outputId": "c0769cdf-340b-408f-ed7b-3d694c4be273"
      },
      "outputs": [],
      "source": [
        "# Cria a instância do MLPBlock\n",
        "mlp_block = MLPBlock(embedding_dim=768, # da Table 1\n",
        "                     mlp_size=3072, # da Table 1\n",
        "                     dropout=0.1) # da Table 3\n",
        "\n",
        "# Passa o MSABlock dentro do MLPBlock\n",
        "patched_image_through_mlp_block = mlp_block(patched_image_through_msa_block)\n",
        "print(f\"Input shape of MLP block: {patched_image_through_msa_block.shape}\")\n",
        "print(f\"Output shape MLP block: {patched_image_through_mlp_block.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rEwejDUZCcnq"
      },
      "source": [
        "### Criando um Transformer Encoder block."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EpaeVED1oWyD"
      },
      "outputs": [],
      "source": [
        "# 1. Cria a classe do nn.Module\n",
        "class TransformerEncoderBlock(nn.Module):\n",
        "    \"\"\"Cria um Transformer Encoder block.\"\"\"\n",
        "    # 2. Inicializa a classe com os hyperparameters da Table 1 e Table 3\n",
        "    def __init__(self,\n",
        "                 embedding_dim:int=768, # Hidden size D da Table 1\n",
        "                 num_heads:int=12, # Heads da Table 1\n",
        "                 mlp_size:int=3072, # MLP size da Table 1\n",
        "                 mlp_dropout:float=0.1, # Quantidade de dropout para dense layers da Table 3\n",
        "                 attn_dropout:float=0): # Quantidade de dropout para attention layers\n",
        "        super().__init__()\n",
        "\n",
        "        # 3. Cria MSA block (equação 2)\n",
        "        self.msa_block = MultiheadSelfAttentionBlock(embedding_dim=embedding_dim,\n",
        "                                                     num_heads=num_heads,\n",
        "                                                     attn_dropout=attn_dropout)\n",
        "\n",
        "        # 4. Cria MLP block (equação 3)\n",
        "        self.mlp_block =  MLPBlock(embedding_dim=embedding_dim,\n",
        "                                   mlp_size=mlp_size,\n",
        "                                   dropout=mlp_dropout)\n",
        "\n",
        "    # 5. Cria o método forward()\n",
        "    def forward(self, x):\n",
        "\n",
        "        # 6. Cria conexão residual para MSA block (adiciona o input para o output)\n",
        "        x =  self.msa_block(x) + x\n",
        "\n",
        "        # 7. Cria conexão residual para MLP block (adiciona o input para o output)\n",
        "        x = self.mlp_block(x) + x\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HKdxpos2DTCe"
      },
      "source": [
        "### Criando a arquitetura do Vision Transformer com os hiperparâmetros do ViT-Base"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hnlz-XkcojAD"
      },
      "outputs": [],
      "source": [
        "# 1. Cria a classe do nn.Module\n",
        "class ViT(nn.Module):\n",
        "    \"\"\"Cria a arquitetura do Vision Transformer com os hiperparâmetros do ViT-Base.\"\"\"\n",
        "    # 2. Inicializa a classe com os hyperparameters da Table 1 e Table 3\n",
        "    def __init__(self,\n",
        "                 img_size:int=224, # Resolução de treino da Table 3\n",
        "                 in_channels:int=3, # Número de canais na imagem de entrada\n",
        "                 patch_size:int=16, # Tammanho do patch\n",
        "                 num_transformer_layers:int=12, # Camadas da Table 1\n",
        "                 embedding_dim:int=768, # Hidden size D da Table 1\n",
        "                 mlp_size:int=3072, # MLP size da Table 1\n",
        "                 num_heads:int=12, # Heads da Table 1\n",
        "                 attn_dropout:float=0, # Dropout para attention projection\n",
        "                 mlp_dropout:float=0.1, # Dropout para dense/MLP layers\n",
        "                 embedding_dropout:float=0.1, # Dropout para patch e position embeddings\n",
        "                 num_classes:int=1000): # Padrão para ImageNet\n",
        "        super().__init__()\n",
        "\n",
        "        # 3. Faça o tamanho da imagem ser divisíveil pelo patch size\n",
        "        assert img_size % patch_size == 0, f\"Image size must be divisible by patch size, image size: {img_size}, patch size: {patch_size}.\"\n",
        "\n",
        "        # 4. Calcula o número de (height * width/patch^2)\n",
        "        self.num_patches = (img_size * img_size) // patch_size**2\n",
        "\n",
        "        # 5. Cria learnable class embedding\n",
        "        self.class_embedding = nn.Parameter(data=torch.randn(1, 1, embedding_dim),\n",
        "                                            requires_grad=True)\n",
        "\n",
        "        # 6. Cria learnable position embedding\n",
        "        self.position_embedding = nn.Parameter(data=torch.randn(1, self.num_patches+1, embedding_dim),\n",
        "                                               requires_grad=True)\n",
        "\n",
        "        # 7. Ccria o valor da embedding dropout\n",
        "        self.embedding_dropout = nn.Dropout(p=embedding_dropout)\n",
        "\n",
        "        # 8. Cria a camada patch embedding\n",
        "        self.patch_embedding = PatchEmbedding(in_channels=in_channels,\n",
        "                                              patch_size=patch_size,\n",
        "                                              embedding_dim=embedding_dim)\n",
        "\n",
        "        # 9. Cria os blocos do Transformer Encoder (podemos criar estacas dos blocos de Transformer Encoder usando nn.Sequential())\n",
        "        # Nota: O \"*\" significa \"todos\"\n",
        "        self.transformer_encoder = nn.Sequential(*[TransformerEncoderBlock(embedding_dim=embedding_dim,\n",
        "                                                                            num_heads=num_heads,\n",
        "                                                                            mlp_size=mlp_size,\n",
        "                                                                            mlp_dropout=mlp_dropout) for _ in range(num_transformer_layers)])\n",
        "\n",
        "        # 10. Cria classifier head\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.LayerNorm(normalized_shape=embedding_dim),\n",
        "            nn.Linear(in_features=embedding_dim,\n",
        "                      out_features=num_classes)\n",
        "        )\n",
        "\n",
        "    # 11. Cria o método forward()\n",
        "    def forward(self, x):\n",
        "\n",
        "        # 12. Obtém batch size\n",
        "        batch_size = x.shape[0]\n",
        "\n",
        "        # 13. Cria a classe token embedding e expande para corresponder ao tamanho do batch equação 1)\n",
        "        class_token = self.class_embedding.expand(batch_size, -1, -1) # \"-1\" siginifica para inferir a dimensão\n",
        "\n",
        "        # 14. Cria patch embedding (equação 1)\n",
        "        x = self.patch_embedding(x)\n",
        "\n",
        "        # 15. Concatena class embedding e patch embedding (equação 1)\n",
        "        x = torch.cat((class_token, x), dim=1)\n",
        "\n",
        "        # 16. Adiciona position embedding no patch embedding (equação 1)\n",
        "        x = self.position_embedding + x\n",
        "\n",
        "        # 17. Roda embedding dropout (Appendix B.1)\n",
        "        x = self.embedding_dropout(x)\n",
        "\n",
        "        # 18. Passa o patch, position e class embedding dentro das camadas do transformer (equações 2 & 3)\n",
        "        x = self.transformer_encoder(x)\n",
        "\n",
        "        # 19. Coloca o index 0 index entre o classifier (equação 4)\n",
        "        x = self.classifier(x[:, 0])\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bVmPPqxOpwq7",
        "outputId": "b25767e8-cdd5-45fd-d3ac-ff96b45f0f73"
      },
      "outputs": [],
      "source": [
        "set_seeds()\n",
        "\n",
        "# Cria um tensor aleatório com o mesmo formato de uma única imagem\n",
        "random_image_tensor = torch.randn(1, 3, 224, 224) # (batch_size, color_channels, height, width)\n",
        "\n",
        "# Cria uma instância do ViT com o número de classes que estamos trabalhando (pizza, steak, sushi)\n",
        "vit = ViT(num_classes=len(class_names))\n",
        "\n",
        "# Passar uma imagem aleatória ao tensor da nossa ViT\n",
        "vit(random_image_tensor)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b837QOT68cNE"
      },
      "source": [
        "## Treinamento"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gyTg9Pk3NEa5",
        "outputId": "34b898ba-e965-4a8c-9681-c705523b49e9"
      },
      "outputs": [],
      "source": [
        "\n",
        "t0 = time.time()\n",
        "_ = next(iter(train_dataloader))\n",
        "print(\"1 batch carregado em\", time.time()-t0, \"s\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 188,
          "referenced_widgets": [
            "bea6e768067943c39d93d3a1b880516a",
            "9b3dc9ef11cf4257896ad26be95a9472",
            "259cb1c454a343eaa6bac39b97cbc31f",
            "34457119011a431c966b58f5cff194ba",
            "d5745afbb9ee454f8f229455407c5c19",
            "f2ba77e470ba4349b72ebee09df06382",
            "e4d59c57be7b4414ba5336de28d3c27a",
            "a1a429c185f14caeab3adbdb157873a7",
            "da477c95262147d88ea31073aa717ba4",
            "e19a78e5838848f1bb0f1e42690e9f55",
            "e9f15a9f62f9484c86a6fd17af23903f"
          ]
        },
        "id": "MteupS7Uo52b",
        "outputId": "0ce2eafd-7351-423d-e473-39cb61268d45"
      },
      "outputs": [],
      "source": [
        "# Configura o otimizador usando os hiperparâmetros vindos do paper\n",
        "optimizer = torch.optim.Adam(params=vit.parameters(),\n",
        "                             lr=3e-3, # Base LR da Table 3 para ViT-* ImageNet-1k\n",
        "                             betas=(0.9, 0.999), # valores padrões mas também mencionados no paper section 4.1 (Training & Fine-tuning)\n",
        "                             weight_decay=0.3) # Do ViT paper section 4.1 (Training & Fine-tuning) e Tabela 3 para ViT-* ImageNet-1k\n",
        "\n",
        "# Configura a função de perda para multi-class classification\n",
        "loss_fn = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "set_seeds()\n",
        "\n",
        "# Treina o modelo e salva os resultados em um dicionário\n",
        "results = train(model=vit,\n",
        "                       train_dataloader=train_dataloader,\n",
        "                       test_dataloader=test_dataloader,\n",
        "                       optimizer=optimizer,\n",
        "                       loss_fn=loss_fn,\n",
        "                       epochs=10,\n",
        "                       device=device)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "MzSbc9X_FU5u",
        "W9A7ZpxYK8Ce",
        "-7SOznPEFNkQ",
        "BIh89nHaGQSj",
        "Gxcm1VmYGVqq",
        "hFfY0oBNiBc2",
        "qyhZB4W6jFYS",
        "QKJZIKn68JGp",
        "b837QOT68cNE"
      ],
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "259cb1c454a343eaa6bac39b97cbc31f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a1a429c185f14caeab3adbdb157873a7",
            "max": 10,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_da477c95262147d88ea31073aa717ba4",
            "value": 8
          }
        },
        "34457119011a431c966b58f5cff194ba": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e19a78e5838848f1bb0f1e42690e9f55",
            "placeholder": "​",
            "style": "IPY_MODEL_e9f15a9f62f9484c86a6fd17af23903f",
            "value": " 8/10 [01:11&lt;00:17,  8.94s/it]"
          }
        },
        "9b3dc9ef11cf4257896ad26be95a9472": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f2ba77e470ba4349b72ebee09df06382",
            "placeholder": "​",
            "style": "IPY_MODEL_e4d59c57be7b4414ba5336de28d3c27a",
            "value": " 80%"
          }
        },
        "a1a429c185f14caeab3adbdb157873a7": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bea6e768067943c39d93d3a1b880516a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_9b3dc9ef11cf4257896ad26be95a9472",
              "IPY_MODEL_259cb1c454a343eaa6bac39b97cbc31f",
              "IPY_MODEL_34457119011a431c966b58f5cff194ba"
            ],
            "layout": "IPY_MODEL_d5745afbb9ee454f8f229455407c5c19"
          }
        },
        "d5745afbb9ee454f8f229455407c5c19": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "da477c95262147d88ea31073aa717ba4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e19a78e5838848f1bb0f1e42690e9f55": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e4d59c57be7b4414ba5336de28d3c27a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e9f15a9f62f9484c86a6fd17af23903f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f2ba77e470ba4349b72ebee09df06382": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
